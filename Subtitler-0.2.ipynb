{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing all neede things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Tk # python gui\n",
    "from tkinter.filedialog import askopenfilename# python gui to select file\n",
    "\n",
    "import re #regular expression\n",
    "import nltk # natural language tool-kit\n",
    "from nltk import sent_tokenize, word_tokenize # tokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords # stopwords\n",
    "from string import punctuation # punctuatios marks\n",
    "import inflect # generating numbers and its word & ordinal form\n",
    "\n",
    "from nltk.corpus import wordnet as wn # wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer #finds root word of given word\n",
    "from nltk.stem import PorterStemmer # generates all possible forms of given root word\n",
    "\n",
    "import json # using api \n",
    "import requests # using api\n",
    "\n",
    "\n",
    "# Language Translator\n",
    "from googletrans import Translator  # Import Translator module from googletrans package\n",
    "\n",
    "translator = Translator() # Create object of Translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gui to select srt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "filename = askopenfilename() # show an \"Open\" dialog box and return the path to the selected file\n",
    "#to see which file is selected\n",
    "#print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read inputDataFile of a string containg  subtitle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile=filename\n",
    "with open (inputFile, \"r\") as myfile:\n",
    "    data=myfile.read()\n",
    "myfile.close()\n",
    "#to see what data contains==> subtitle\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove other than subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = re.sub(\"\\d+|:|,|-->|\\n|<.*?>|'s\", \"\", data)\n",
    "data = re.sub(\"-\", \" \", data)\n",
    "#to see length of data after removing some special characters\n",
    "#len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert data into word form and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(map(str.lower, word_tokenize(data)))\n",
    "newData=set()\n",
    "for i in data:\n",
    "    newData.add(i)\n",
    "#data contains words of subtitle and it's a set    \n",
    "data=set()\n",
    "data=newData\n",
    "#to see tokenized words \n",
    "#print(data,len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove stopWords and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toBeRemoved contains words that are stopwords and punctuations that we want to remove from data\n",
    "toBeRemoved = set(stopwords.words('english'))\n",
    "toBeRemoved = toBeRemoved.union(set(punctuation))\n",
    "#punctuaitons\n",
    "#print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more words to remove as stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_json = {\"en\":[\"a\",\"n't\",\"'re'\",\"'ve'\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "toBeRemoved = toBeRemoved.union(set(stopwords_json['en']))\n",
    "# to see new list after ading some few stopwords\n",
    "#print(toBeRemoved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contraction that is of no use like stopWords \n",
    "### so remove'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractionSet=set()\n",
    "for key in contractions:\n",
    "    contractionSet.add(key)\n",
    "toBeRemoved = toBeRemoved.union(contractionSet)\n",
    "#to see new list of words to be removed from subtitle\n",
    "#print(toBeRemoved,len(toBeRemoved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number in form of words --toBe removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "numberth=[p.ordinal(p.number_to_words(i)) for i in range(21)]\n",
    "tenth=[p.ordinal(p.number_to_words(i)) for i in range(10,101,10)]\n",
    "tens=[p.number_to_words(i) for i in range(10,101,10)]\n",
    "numbers=[p.number_to_words(i) for i in range(21)]\n",
    "oneZeros=[p.number_to_words(10**i) for i in range(21)]\n",
    "oneZeroth=[p.ordinal(p.number_to_words(10**i)) for i in range(21)]\n",
    "number=set()\n",
    "\n",
    "for i in numberth:\n",
    "    number.add(i)\n",
    "for i in numbers:\n",
    "    number.add(i)\n",
    "for i in tenth:\n",
    "    number.add(i)\n",
    "for i in tens:\n",
    "    number.add(i)\n",
    "for i in oneZeros:\n",
    "    for j in i.split():\n",
    "        number.add(j)\n",
    "for i in oneZeroth:\n",
    "    for j in i.split():\n",
    "        number.add(j)\n",
    "\n",
    "toBeRemoved =toBeRemoved.union(number)\n",
    "#to see newly added numbers|word form|ordinal form\n",
    "#print(toBeRemoved,len(toBeRemoved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Remove stop words\n",
    "dataWithoutStopWords=list(filter(lambda w: not w in toBeRemoved,data))\n",
    "#to see all words after removing all kinds of stopwords\n",
    "#print(dataWithoutStopWords,len(dataWithoutStopWords))\n",
    "###Conclusion\n",
    "#(5950-3254)=2696 stopWords removed\n",
    "\n",
    "### 10/8/18\n",
    "## 5950-3274=2676  removed\n",
    "# 5902-3226=2676\n",
    "\n",
    "###16/8/18\n",
    "##1492-1220=272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read commonWords from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = askopenfilename() # show an \"Open\" dialog box and return the path to the selected file\n",
    "with open(filename, 'r') as myfile1:\n",
    "    data1=myfile1.read().replace('\\n', ' ')# read file a replace newLine with space so that split can be performed\n",
    "myfile1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate all possible forms of word from commonWords \n",
    "## and\n",
    "# Remove common words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonWordsArray=data1.split()\n",
    "commonWordsArray=[x.upper() for x in commonWordsArray] #capitalize'em\n",
    "\n",
    "dataWithoutStopWords=[x.upper() for x in dataWithoutStopWords] #capitalize'em\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "p = inflect.engine()\n",
    "\n",
    "forms = set() #We'll store the derivational forms in a set to eliminate duplicates\n",
    "for commonWord in commonWordsArray:\n",
    "    for word in wn.lemmas(commonWord): #for each commonWord lemma in WordNet\n",
    "        forms.add(word.name()) #add the lemma itself\n",
    "        for relatedWord in word.derivationally_related_forms(): #for each related lemma\n",
    "            forms.add(relatedWord.name()) #add the related lemma\\\n",
    "\n",
    "    for word in wn.lemmas(lemmatizer.lemmatize(commonWord)):\n",
    "        forms.add(word.name()) #add the lemma itself\n",
    "        for relatedWord in word.derivationally_related_forms(): #for each related lemma\n",
    "            forms.add(relatedWord.name()) #add the related lemma\\\n",
    "\n",
    "    for word in wn.lemmas(ps.stem(commonWord)):\n",
    "        forms.add(word.name()) #add the lemma itself\n",
    "        for relatedWord in word.derivationally_related_forms(): #for each related lemma\n",
    "            forms.add(relatedWord.name()) #add the related lemma\\\n",
    "\n",
    "    forms.add(lemmatizer.lemmatize(commonWord))\n",
    "    forms.add(commonWord)\n",
    "    forms.add(ps.stem(commonWord))\n",
    "    forms.add(p.plural(commonWord))\n",
    "    \n",
    "forms=[x.upper() for x in forms]# capitalize'em\n",
    "\n",
    "dataWithoutCommonWordsAndItsForms=list(filter(lambda w: not w in forms,dataWithoutStopWords))\n",
    "#to see sophisticatedly removed commonwords from subtitle\n",
    "#print(dataWithoutCommonWordsAndItsForms,len(dataWithoutCommonWordsAndItsForms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#to see length of words remaining\n",
    "#print(len(dataWithoutCommonWordsAndItsForms))\n",
    "###conclusion :9/8/18 :thursday \n",
    "# 3254-1650=1604 common words removed\n",
    "\n",
    "###10/8/18 friday\n",
    "# 3254-1409=1845 common words removed\n",
    "\n",
    "# 3 things added\n",
    "# 3274-1443=1831 removed\n",
    "\n",
    "###11/8/18\n",
    "# inflect plurals\n",
    "# 3274-1333=1941 removed\n",
    "\n",
    "###\n",
    "##1220-291=929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalWordSet=set()\n",
    "for i in range(len(dataWithoutCommonWordsAndItsForms)):\n",
    "    finalWordSet.add(dataWithoutCommonWordsAndItsForms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in finalWordSet:\n",
    "    if len(i)>2:\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "finalNewWords=[]\n",
    "for i in finalWordSet:\n",
    "    if len(i)>3:\n",
    "        finalNewWords.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=\"/home/sarkijatru/Desktop/documentry_Saturday/newWords.txt\"\n",
    "newWords=open(temp,\"w\")\n",
    "\n",
    "#str = ''.join(str(i) for i in finalWordSet)\n",
    "for i in finalNewWords:\n",
    "    newWords.write(i)\n",
    "    newWords.write('\\n')\n",
    "newWords.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colour all new words in subtitle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idea 1:\n",
    "    read those new words\n",
    "    find'em from subtitle \n",
    "    add code for applying color "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(temp,\"r\") as w:\n",
    "    words=w.readlines()\n",
    "\n",
    "wordSet=[i.replace('\\n','') for i in words]\n",
    "w.close()\n",
    "\n",
    "# with open (inputFile, \"r\") as myfile:\n",
    "#     subtitle=myfile.read().upper()\n",
    "\n",
    "with open (inputFile, \"r\") as myfile:\n",
    "    subtitle=myfile.readlines()\n",
    "myfile.close()\n",
    "\n",
    "newColor=\"#FFFF00\"#blue\n",
    "oldColor=\"#CCCCCC\"#old\n",
    "\n",
    "zeroOrMore=\"[\\s|\\S]*\"\n",
    "oneOrMore=\"[\\s|\\S]+\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(subtitle)):\n",
    "    for eachWord in re.findall(r'\\b\\w+\\b',subtitle[i]):\n",
    "        if not eachWord.isdecimal():# only for words-> excluding intergers\n",
    "            if eachWord.upper() in wordSet:\n",
    "                #to see eachWord selected\n",
    "                #print(eachWord)\n",
    "                patternBefore=zeroOrMore+eachWord+zeroOrMore+\"<font\"+zeroOrMore+\"</font>\"+zeroOrMore\n",
    "                patternAfter=zeroOrMore+\"<font\"+zeroOrMore+\"</font>\"+zeroOrMore+eachWord+zeroOrMore\n",
    "                patternBetween=zeroOrMore+\"<font\"+oneOrMore+eachWord+zeroOrMore+\"</font>\"+zeroOrMore\n",
    "                \n",
    "                if re.match(patternBetween,subtitle[i],re.IGNORECASE):\n",
    "                    newReplacement=\"</font><font color=\\\"\"+newColor+\"\\\"> \"+eachWord.upper()+\"</font><font>\"\n",
    "                    #to see lable \n",
    "                    #print(\"between\")\n",
    "                elif re.match(patternBefore,subtitle[i],re.IGNORECASE):\n",
    "                    newReplacement=\"<font color=\\\"\"+newColor+\"\\\"> \"+eachWord.upper()+\"</font>\"\n",
    "                    #to see lable \n",
    "                    #print(\"Before\")\n",
    "                elif re.match(patternAfter,subtitle[i],re.IGNORECASE):\n",
    "                    newReplacement=\"<font color=\\\"\"+newColor+\"\\\"> \"+eachWord.upper()+\"</font>\"\n",
    "                    #to see lable \n",
    "                    #print(\"After\")\n",
    "                \n",
    "                else:\n",
    "                    newReplacement=\"<font color=\\\"\"+newColor+\"\\\"> \"+eachWord.upper()+\"</font>\"\n",
    "                    #to see lable \n",
    "                    #print(\"nothing\")\n",
    "                \n",
    "                \n",
    "#                 if \"font\" in subtitle[i]:# finding particular word \n",
    "#                     if newColor in subtitle[i]:\n",
    "#                         newReplacement=\"<font color=\\\"\"+newColor+\"\\\"> \"+eachWord.upper()+\"</font>\"\n",
    "#                     else:\n",
    "#                         newReplacement=\"</font><font color=\\\"\"+newColor+\"\\\"> \"+eachWord.upper()+\"</font><font>\"\n",
    "#                 else:\n",
    "#                     newReplacement=\"<font color=\\\"\"+newColor+\"\\\"> \"+eachWord.upper()+\"</font>\"\n",
    "                    \n",
    "                        \n",
    "\n",
    "                subtitle[i]=subtitle[i].replace(eachWord,newReplacement)\n",
    "                #to see changed subtitle for given word\n",
    "                #print(subtitle[i])\n",
    "#to see the list of new words\n",
    "#print(wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile=inputFile.replace('.srt','(processed).srt')\n",
    "output=open(inputFile,\"w\")\n",
    "#output.write(subtitle.lower())\n",
    "for eachLine in subtitle:\n",
    "    output.write(eachLine)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd project for the same\n",
    "## get sentences for above words from an api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "document = Document()\n",
    "\n",
    "document.add_heading('Documentry Today', 0)\n",
    "\n",
    "p = document.add_paragraph('new words for you ')\n",
    "p.add_run('to know ').bold = True\n",
    "p.add_run(' and ')\n",
    "p.add_run('practise.').italic = True\n",
    "\n",
    "document.add_heading('LEARN', level=1)\n",
    "document.add_paragraph('learning english made efficient', style='Intense Quote')\n",
    "\n",
    "\n",
    "temp=\"/home/sarkijatru/Desktop/documentry_Saturday/newWords.txt\"\n",
    "with open(temp,\"r\") as w:\n",
    "    words=w.readlines()\n",
    "\n",
    "newWordList=[i.replace('\\n','') for i in words]\n",
    "w.close()\n",
    "\n",
    "# to store word and its meaning in tabular form\n",
    "table = document.add_table(rows=1, cols=2)\n",
    "hdr_cells = table.rows[0].cells\n",
    "hdr_cells[0].text = 'word'\n",
    "hdr_cells[1].text = 'meaning'\n",
    "\n",
    "\n",
    "for eachWord in newWordList:\n",
    "    row_cells = table.add_row().cells\n",
    "    row_cells[0].text = eachWord\n",
    "\n",
    "    translator = Translator() # Create object of Translator.\n",
    "\n",
    "    translated = translator.translate(eachWord.lower(), src='en', dest='gu') # Pass both source and destination\n",
    "    \n",
    "    row_cells[1].text = translated.text\n",
    "    print(eachWord,\"\\t\",translated.text)\n",
    "document.add_page_break()\n",
    "\n",
    "document.save('new words list.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tkinter import *\n",
    "# from tkinter import messagebox\n",
    "# tkinter.messagebox\n",
    "# from tkinter import filedialog    \n",
    "# def donothing():\n",
    "#    print(\"a\")\n",
    "\n",
    "# def file_save():\n",
    "#     f = filedialog.asksaveasfile(mode='w', defaultextension=\".txt\")\n",
    "#     if f is None: # asksaveasfile return `None` if dialog closed with \"cancel\".\n",
    "#         return\n",
    "#     text2save = str(text.get(1.0, END)) # starts from `1.0`, not `0.0`\n",
    "#     f.write(text2save)\n",
    "#     f.close() # `()` was missing.\n",
    "\n",
    "# root = Tk()\n",
    "# root.geometry(\"500x500\")\n",
    "# menubar=Menu(root)\n",
    "# text=Text(root)\n",
    "# text.pack()\n",
    "# filemenu=Menu(menubar,tearoff=0)\n",
    "# filemenu.add_command(label=\"New\", command=donothing)\n",
    "# filemenu.add_command(label=\"Open\", command=donothing)\n",
    "# filemenu.add_command(label=\"Save\", command=file_save)\n",
    "# filemenu.add_command(label=\"Save as...\", command=donothing)\n",
    "# filemenu.add_command(label=\"Close\", command=donothing)\n",
    "# filemenu.add_separator()\n",
    "# filemenu.add_command(label=\"Exit\", command=root.quit)\n",
    "# menubar.add_cascade(label=\"File\", menu=filemenu)\n",
    "\n",
    "# editmenu=Menu(menubar,tearoff=0)\n",
    "# editmenu.add_command(label=\"Undo\", command=donothing)\n",
    "# editmenu.add_command(label=\"Copy\", command=donothing)\n",
    "# editmenu.add_command(label=\"Paste\", command=donothing)\n",
    "# menubar.add_cascade(label=\"Edit\", menu=editmenu)\n",
    "\n",
    "# helpmenu=Menu(menubar,tearoff=0)\n",
    "# helpmenu.add_command(label=\"Help\",command=donothing)\n",
    "# file_save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
