{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read inputDataFile of a string containg sentences of a subtitle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"/home/sarkijatru/Desktop/srikarProject/usingLex/normal/data.txt\", \"r\") as myfile:\n",
    "    data=myfile.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "\n",
    "## Replace shortWord with original form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in data[0].split():\n",
    "    if word.lower() in contractions:\n",
    "        #data[0] = data[0].replace(word, contractions[word.lower()])\n",
    "        ##edited on 11/8/18\n",
    "        data[0] = data[0].replace(word,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regexp to replace / with space''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data[0]= re.sub(r'/', \" \", data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split string  to get words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5602"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataArray=data[0].split()\n",
    "len(dataArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3205"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "s=set(stopwords.words('english'))\n",
    "### Remove stop words\n",
    "wordsWithoutStopWords=list(filter(lambda w: not w in s,dataArray))\n",
    "len(wordsWithoutStopWords)\n",
    "\n",
    "###Conclusion\n",
    "#(5950-3254)=2696 stopWords removed\n",
    "\n",
    "### 10/8/18\n",
    "## 5950-3274=2676  removed\n",
    "# 5902-3226=2676"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read commonWords from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sarkijatru/Desktop/srikarProject/commonWords.txt', 'r') as myfile1:\n",
    "    data1=myfile1.read().replace('\\n', ' ')# read file a replace newLine with space so that split can be performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate all possible forms of word from commonWords \n",
    "## and\n",
    "# Remove common words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonWordsArray=data1.split()\n",
    "commonWordsArray=[x.upper() for x in commonWordsArray] #capitalize'em\n",
    "\n",
    "wordsWithoutStopWords=[x.upper() for x in wordsWithoutStopWords] #capitalize'em\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "forms = set() #We'll store the derivational forms in a set to eliminate duplicates\n",
    "for commonWord in commonWordsArray:\n",
    "    for word in wn.lemmas(commonWord): #for each commonWord lemma in WordNet\n",
    "        forms.add(word.name()) #add the lemma itself\n",
    "        for relatedWord in word.derivationally_related_forms(): #for each related lemma\n",
    "            forms.add(relatedWord.name()) #add the related lemma\\\n",
    "\n",
    "    for word in wn.lemmas(lemmatizer.lemmatize(commonWord)):\n",
    "        forms.add(word.name()) #add the lemma itself\n",
    "        for relatedWord in word.derivationally_related_forms(): #for each related lemma\n",
    "            forms.add(relatedWord.name()) #add the related lemma\\\n",
    "\n",
    "    for word in wn.lemmas(ps.stem(commonWord)):\n",
    "        forms.add(word.name()) #add the lemma itself\n",
    "        for relatedWord in word.derivationally_related_forms(): #for each related lemma\n",
    "            forms.add(relatedWord.name()) #add the related lemma\\\n",
    "\n",
    "    forms.add(lemmatizer.lemmatize(commonWord))\n",
    "    forms.add(commonWord)\n",
    "    forms.add(ps.stem(commonWord))\n",
    "    forms.add(p.plural(commonWord))\n",
    "    \n",
    "forms=[x.upper() for x in forms]# capitalize'emforms=[x.upper() for x in forms]# capitalize'em\n",
    "\n",
    "wordsWithoutCommonWordsAndItsForms=list(filter(lambda w: not w in forms,wordsWithoutStopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordsWithoutCommonWordsAndItsForms)\n",
    "###conclusion :9/8/18 :thursday \n",
    "# 3254-1650=1604 common words removed\n",
    "\n",
    "###10/8/18 friday\n",
    "# 3254-1409=1845 common words removed\n",
    "\n",
    "# 3 things added\n",
    "# 3274-1443=1831 removed\n",
    "\n",
    "###11/8/18\n",
    "# inflect plurals\n",
    "# 3274-1333=1941 removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalWordSet=set()\n",
    "for i in range(len(wordsWithoutCommonWordsAndItsForms)):\n",
    "    finalWordSet.add(wordsWithoutCommonWordsAndItsForms[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABERRATION',\n",
       " 'ABOARD',\n",
       " 'AGROUND',\n",
       " 'AIMING',\n",
       " 'AIRMEN',\n",
       " 'AIRSEA',\n",
       " 'AIRTRAFFIC',\n",
       " 'AKIN',\n",
       " 'ALIGNED',\n",
       " 'ANOMALIES',\n",
       " 'ANOR',\n",
       " \"ANYBODY'S\",\n",
       " 'APPROACHED',\n",
       " 'ARRIVING',\n",
       " 'ARTIFACTS',\n",
       " 'ASHORE',\n",
       " 'ASTONISHING',\n",
       " 'ATTRACTED',\n",
       " 'ATTRACTING',\n",
       " 'ATTRIBUTED',\n",
       " 'AVENGER',\n",
       " 'BAFFLING',\n",
       " 'BEACHGOERS',\n",
       " 'BERMUDIAN',\n",
       " 'BESIEGED',\n",
       " 'BLAMED',\n",
       " 'BLASTING',\n",
       " 'BLEW',\n",
       " 'BOHEMIANS',\n",
       " 'BOMBERS',\n",
       " 'BORDERED',\n",
       " 'BREW',\n",
       " 'BUOYANCY',\n",
       " 'BUOYANT',\n",
       " 'CALLSIGN',\n",
       " 'CELEBRATED',\n",
       " 'CHOPPY',\n",
       " 'CINCHY',\n",
       " 'CIRCULATED',\n",
       " 'COASTGUARD',\n",
       " 'COASTLINE',\n",
       " 'COLLIER',\n",
       " 'COLONIZATION',\n",
       " 'COLONIZED',\n",
       " 'COMPASS',\n",
       " 'COMPASSES',\n",
       " 'CONDENSES',\n",
       " 'COPILOT',\n",
       " 'COUNTERCLOCKWISE',\n",
       " 'COUNTLESS',\n",
       " 'COURTMARTIALED',\n",
       " 'CRASHING',\n",
       " 'CREDENCE',\n",
       " 'CREDIBILITY',\n",
       " 'CREWMAN',\n",
       " 'CYCLOPS',\n",
       " 'DECEPTIVELY',\n",
       " 'DEEPEST',\n",
       " 'DEFY',\n",
       " 'DESOLATE',\n",
       " 'DEVASTATED',\n",
       " \"DEVIL'S\",\n",
       " 'DINARDO',\n",
       " 'DISAPPEAR',\n",
       " 'DISAPPEARANCE',\n",
       " 'DISAPPEARANCES',\n",
       " 'DISAPPEARED',\n",
       " 'DISAPPEARS',\n",
       " 'DISCREPANCIES',\n",
       " 'DISGUISE',\n",
       " 'DISORIENTATION',\n",
       " 'DISTRESS',\n",
       " 'DITCH',\n",
       " 'DITCHED',\n",
       " 'DITCHING',\n",
       " 'DOTTED',\n",
       " 'DOWNBURST',\n",
       " 'DOWNBURSTS',\n",
       " 'DOWNPOUR',\n",
       " 'DRAINED',\n",
       " 'DROPOFF',\n",
       " 'DROPPING',\n",
       " \"EDD'S\",\n",
       " \"EDDIE'S\",\n",
       " 'ELECTROMAGNETIC',\n",
       " 'ELECTRONICAL',\n",
       " 'EMANATE',\n",
       " 'EMERGED',\n",
       " 'ENDEMIC',\n",
       " 'ENGULFED',\n",
       " 'ENROUTE',\n",
       " 'ERUPT',\n",
       " 'ERUPTION',\n",
       " 'ERUPTIONS',\n",
       " 'EVACUATE',\n",
       " 'EXPANSE',\n",
       " 'EXPLAINABLE',\n",
       " 'EXQUISITE',\n",
       " 'EXTINCT',\n",
       " 'FAINT',\n",
       " \"FAR'S\",\n",
       " 'FASCINATED',\n",
       " 'FEARFUL',\n",
       " 'FISHERMAN',\n",
       " 'FISHERMEN',\n",
       " 'FLARES',\n",
       " 'FLEW',\n",
       " 'FLIERS',\n",
       " 'FLIES',\n",
       " 'FLIGHTED',\n",
       " 'FOLLOWLEADER',\n",
       " 'FONTFONT',\n",
       " 'FRANTIC',\n",
       " 'FREAK',\n",
       " 'FRINGED',\n",
       " 'FURR',\n",
       " 'GALLONS',\n",
       " \"GARNON'S\",\n",
       " 'GEEZ',\n",
       " 'GERNON',\n",
       " 'GIAN',\n",
       " 'GRAVEYARD',\n",
       " 'GREETED',\n",
       " 'GUESSWORK',\n",
       " 'GUTS',\n",
       " 'HAILING',\n",
       " 'HANAN',\n",
       " 'HAYWIRE',\n",
       " 'HAZE',\n",
       " 'HEARTBEAT',\n",
       " 'HEAVIER',\n",
       " 'HELPLESS',\n",
       " 'HEN',\n",
       " 'HOPELESSLY',\n",
       " \"HOUR'S\",\n",
       " 'HOURANDAHALF',\n",
       " 'HOVERING',\n",
       " 'IHNEN',\n",
       " 'INEQUITY',\n",
       " 'INEXPLICABLE',\n",
       " 'INFAMOUS',\n",
       " 'INHABIT',\n",
       " 'INHABITED',\n",
       " \"INSTITUTE'S\",\n",
       " 'IRONICALLY',\n",
       " 'IRRATIONAL',\n",
       " 'ISLANDERS',\n",
       " 'KAZAR',\n",
       " 'KIDNAPPING',\n",
       " 'KISSED',\n",
       " 'LANDED',\n",
       " 'LASHED',\n",
       " 'LAUGHTER',\n",
       " 'LEFTHAND',\n",
       " 'LENTICULAR',\n",
       " 'LIEUTENANT',\n",
       " 'LOCALIZED',\n",
       " 'LOZ',\n",
       " 'MAHIMAHI',\n",
       " 'MALFUNCTIONED',\n",
       " 'MALFUNCTIONING',\n",
       " 'MANEUVER',\n",
       " 'MARINER',\n",
       " 'MARLIN',\n",
       " 'MAYDAY',\n",
       " 'MAYDAYS',\n",
       " 'MEANTIME',\n",
       " 'METEOROLOGISTS',\n",
       " 'METHANE',\n",
       " \"MIAMI'S\",\n",
       " 'MICROBURST',\n",
       " 'MIST',\n",
       " 'MONTEREY',\n",
       " 'MORAN',\n",
       " 'MUTA',\n",
       " 'MYSTERIES',\n",
       " 'MYSTERIOUSLY',\n",
       " 'NAUTICAL',\n",
       " \"NAVY'S\",\n",
       " 'NEIR',\n",
       " 'NINETEENYEAROLD',\n",
       " 'NINETY',\n",
       " 'NORRNMOST',\n",
       " 'NOTORIETY',\n",
       " 'NOTORIOUS',\n",
       " 'NUTSHELL',\n",
       " 'ONETHIRD',\n",
       " 'ORETICALLY',\n",
       " 'ORIES',\n",
       " 'ORY',\n",
       " 'PANICSTRICKEN',\n",
       " 'PECULIAR',\n",
       " 'PERCHED',\n",
       " 'PERK',\n",
       " 'PERPLEXING',\n",
       " 'PICTURED',\n",
       " 'PLAUSIBLE',\n",
       " 'PMFONT',\n",
       " 'POSTGRADUATE',\n",
       " 'PRECARIOUSLY',\n",
       " 'PRISTINE',\n",
       " \"PUBLIC'S\",\n",
       " 'PUREST',\n",
       " 'QUIZAR',\n",
       " 'RAFT',\n",
       " 'RAR',\n",
       " \"RE'LL\",\n",
       " \"RE'S\",\n",
       " 'REACTED',\n",
       " 'REALLIFE',\n",
       " 'RESEARCHING',\n",
       " 'RIG',\n",
       " 'RIGHTHAND',\n",
       " 'ROTATING',\n",
       " 'ROTHSCHILD',\n",
       " 'ROTHSCHILDS',\n",
       " 'RUMORED',\n",
       " 'RUMORS',\n",
       " \"SAILOR'S\",\n",
       " 'SAILORS',\n",
       " 'SCATTERED',\n",
       " 'SCREAMING',\n",
       " 'SCREAMS',\n",
       " 'SEABOARD',\n",
       " 'SEARCHANDRESCUE',\n",
       " 'SEEMINGLY',\n",
       " 'SEISMIC',\n",
       " 'SEIZE',\n",
       " 'SERIE',\n",
       " 'SHALLOW',\n",
       " 'SHIPWRECKS',\n",
       " 'SHOALS',\n",
       " 'SHORTED',\n",
       " 'SHORTING',\n",
       " 'SHUDDERING',\n",
       " 'SHUTTERING',\n",
       " 'SIGNALED',\n",
       " 'SINKINGS',\n",
       " 'SKEPTICISM',\n",
       " 'SKIES',\n",
       " 'SLOANE',\n",
       " 'SMARTER',\n",
       " 'SOAR',\n",
       " 'SOS',\n",
       " 'SOUASTERN',\n",
       " 'SPACETIME',\n",
       " 'SPAWNED',\n",
       " 'SPECULATION',\n",
       " 'SPOTTED',\n",
       " 'SPOUTS',\n",
       " 'STARBOARD',\n",
       " 'STATUE',\n",
       " 'STEEPLY',\n",
       " 'SUBTROPICAL',\n",
       " 'SUNBAKED',\n",
       " 'SUNK',\n",
       " 'SUNKEN',\n",
       " 'SURVIVED',\n",
       " 'SWEEP',\n",
       " 'SWELLS',\n",
       " 'SWERVED',\n",
       " 'SYLVIA',\n",
       " 'TBM',\n",
       " 'TERRIFIED',\n",
       " 'THREEMAN',\n",
       " 'THUNDERSTORM',\n",
       " 'THUNDERSTORMS',\n",
       " 'TOGER',\n",
       " 'TORNADIC',\n",
       " 'TORNADOES',\n",
       " 'TRANSMUTATIONS',\n",
       " 'TRAVELED',\n",
       " 'TREACHEROUS',\n",
       " 'TROLL',\n",
       " 'UFOS',\n",
       " 'UNDERSEA',\n",
       " 'UNDERWATER',\n",
       " 'UNFOLDING',\n",
       " 'UNPREDICTABLE',\n",
       " 'USS',\n",
       " 'UTTERLY',\n",
       " 'VANISH',\n",
       " 'VANISHED',\n",
       " 'VEER',\n",
       " 'VENTURED',\n",
       " 'VERTIGO',\n",
       " 'VICINITY',\n",
       " 'VIOLENTLY',\n",
       " \"VISITOR'S\",\n",
       " 'VOLCANO',\n",
       " 'VORTEX',\n",
       " 'WAHOO',\n",
       " 'WAKEFIELD',\n",
       " 'WARP',\n",
       " 'WARPS',\n",
       " 'WARY',\n",
       " 'WATERSPORTS',\n",
       " 'WATERSPOUT',\n",
       " 'WATERSPOUTS',\n",
       " 'WEAKENED',\n",
       " 'WELLKNOWN',\n",
       " 'WESTWARDS',\n",
       " 'WHER',\n",
       " 'WHIP',\n",
       " 'WOODROW',\n",
       " \"WORLD'S\",\n",
       " 'WORSENING',\n",
       " 'WORSHIPERS',\n",
       " 'WRECKAGE',\n",
       " 'WRITHING',\n",
       " 'XIN',\n",
       " \"Y'RE\",\n",
       " \"Y'VE\",\n",
       " 'YELLED'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalWordSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalWordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
